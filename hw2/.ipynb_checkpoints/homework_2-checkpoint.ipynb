{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS585 Homework 2: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is due on October 12, 2018, submitted electronically. 100 points total.\n",
    "\n",
    "In this homework, you will complete the following tasks: \n",
    "1. implement the objective function and parameter updates for word2vec's skipgram with negative sampling (SGNS) algorithm. \n",
    "2. answer questions about the training process and produce a visualization of some pretrained word embeddings\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? Either Python 2 or 3 should work\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out a supporting file, `word2vec.py`.\n",
    "\n",
    "- For all of the textual answers you have to fill out have placeholder text which says \"Write your answer here\" For each question, you need to replace \"Write your answer here\" with your answer.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope (2) Turn in `word2vec.py` and `homework_2.ipynb` on Moodle.\n",
    "  \n",
    "- **Important** check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipython notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf 2018hw2.ipynb` to identify and fix any syntax errors that might be causing problems\n",
    "\n",
    "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " \n",
    "- This assignment is designed so that you can run all cells in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a set number of students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is also considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: toy word2vec (65 points)\n",
    "\n",
    "Our first task will be to implement SGNS. We'll be working with a small \"toy\" dataset to ensure that your model works properly before moving on to real text. To start, run the cell below to set up the toy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function\n",
    "from word2vec import *\n",
    "import numpy as np\n",
    "\n",
    "# first task: develop a working word2vec model on a toy dataset\n",
    "\n",
    "# we'll set some simple model hyperparameters...\n",
    "dim = 5 # dimensionality of word vectors\n",
    "window_size = 4 # how many total surrounding words to include in context\n",
    "vocab_size = 10 # number of word types in vocab\n",
    "vocab = range(vocab_size) # small vocab for developing our model\n",
    "\n",
    "# now let's initialize our word embedding matrices\n",
    "W,C = init_parameters(dim, vocab_size)\n",
    "print(W.shape, C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 (5 pts):\n",
    "What does each row of W represent? What about each column?\n",
    "\n",
    "\n",
    "**write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2 (15 pts): \n",
    "The *compute_obj_and_grad* function in *word2vec.py* partially implements the SGNS objective function for a single (target word, context word) pair: $L = \\log P(+ | t,c) + \\sum_{i=0}^k \\log P(- |t, n_i)$. \n",
    "\n",
    "You will complete this function by filling in the *compute_pos_prob* and *compute_neg_prob* functions. Specifically, *compute_pos_prob* should compute $P(+ | t,c)$ for a target word vector $t$ and a context word vector $c$, while *compute_neg_prob* should compute $P(- |t, n_i)$ for a negative sample word vector $n_i$ and $c$. \n",
    "\n",
    "***IMPORTANT:*** Please do not implement your own sigmoid function! word2vec.py imports scipy's sigmoid function, which you can use simply by typing *sigmoid*, as in ```x = sigmoid(5)```\n",
    "\n",
    "If you did it correctly, running the cell below will give you a \"congratulations\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_check(compute_obj_and_grad, dim, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 (5 pts):\n",
    "Take a look at the *dW* and *dC* variables in the *compute_obj_and_grad* function, which will eventually contain $\\frac{\\partial L}{\\partial{W}}$ and $\\frac{\\partial L}{\\partial{C}}$, respectively. They are matrices of the same size as W and C. What should each row of *dW* contain? What about each row of *dC*? \n",
    "\n",
    "**write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 (40 pts): \n",
    "Implement the gradient of the SGNS objective by properly updating the *dW* and *dC* variables in the *compute_obj_and_grad* function. You'll need to compute four different partial derivatives, each of which is marked by \"IMPLEMENT ME\" in the code:\n",
    "1. derivative of the log P(+ | t, c) term of L WRT target word vector t\n",
    "2. derivative of the log P(+ | t, c) term of L WRT context word vector c\n",
    "3. derivative of the log P(- | t, n_i) term of L WRT target word vector t\n",
    "4. derivative of the log P(- | t, n_i) term of L WRT context word vector n_i\n",
    "\n",
    "If you do it correctly, you will see a \"congratulations\" message after running the next cell.\n",
    "\n",
    "***Hint:*** You may find it convenient to reuse the *pos_prob* and *neg_prob* variables in your derivative computations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "performs a gradient check using a finite differences approximation \n",
    "(if interested, see https://en.wikipedia.org/wiki/Finite_difference). \n",
    "the function feeds target word 3 and context word 9 as inputs, and \n",
    "computes the gradient using words [4,3,2,2] as negative samples. \n",
    "if there are any errors in your code, the function will print out which\n",
    "target and context word derivatives were improperly computed.\n",
    "''' \n",
    "gradient_check(compute_obj_and_grad, W, C, dim, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: visualizing word2vec (35 pts)\n",
    "\n",
    "Now that we have a working SGNS algorithm, we'll first take a look at the training process before moving on to evaluating some pretrained embeddings. Run the next cell to read in a small subset of Wikipedia and reset the model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the small data file\n",
    "data_file = 'small_text8'\n",
    "num_epochs = 3 # number of passes through the dataset to make\n",
    "text, vocab, sampling_dist = compute_vocab(data_file) \n",
    "idx_to_w = dict((v,k) for (k,v) in vocab.items()) # useful for eval / debugging; if you are using Python2, replace vocab.items() by vocab.iteritems()\n",
    "\n",
    "# onto some real-world model hyperparameters...\n",
    "dim = 25 # dimensionality of word vectors\n",
    "window_size = 4 # how many total surrounding words to include in context\n",
    "vocab_size = len(vocab) # number of word types in vocab\n",
    "\n",
    "W, C = init_parameters(dim, vocab_size) # new embeddings\n",
    "print(W.shape, C.shape, len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 (5 pts):\n",
    "You should see that W and C are both of shape (52754, 25), while the *text* variable contains 1000000 tokens from Wikipedia. Now, let's analyze the *sampling_dist* variable, which is a list of words from which negative samples are drawn. Write some code below to print out the top 20 most frequently occurring words in *sampling_dist*. Use the *idx_to_w* dictionary to map from indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT ME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2 (5 pts):\n",
    "Your list should be dominated by common words (e.g., \"the\", \"and\", \"of\"). Why is this expected? \n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3 (5 pts):\n",
    "Examine the *train* function in word2vec.py. Is it performing gradient ascent or descent? How can you tell? \n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this implementation of SGNS is horribly slow and will take days to converge even on this tiny dataset :( Google's implementation is in highly-optimized C (https://github.com/dav/word2vec/blob/master/src/word2vec.c). Instead of making you train your own embeddings, we'll take a look at some pretrained word embeddings! Run the below cell to load 50-d GloVe embeddings for every word in our vocabulary. This should take a couple of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = load_embeddings('glove.6B.50d.txt', vocab, idx_to_w)\n",
    "print('loaded pretrained embeddings of shape %s' % repr(W.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4 (5 pts):\n",
    "Now let's examine the learned embeddings using nearest neighbors. We have provided you an efficient function that prints nearest neighbors using cosine distance. First run the below cell to identify the nearest neighbors of the words *baseball* and *bad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors('baseball', vocab, idx_to_w, W)\n",
    "nearest_neighbors('bad', vocab, idx_to_w, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now provide the part-of-speech tag(s) for each of the 10 nearest neighbors of *bad* using the Penn Treebank POS tagset (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). Additionally, which of *bad*'s nearest neighbors do you think shouldn't be in this list (if any)? Justify your answer. \n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5 (5 pts):\n",
    "Now we're going to visualize a subset of these word embeddings. First, we'll have to compute a 2-dimensional projection of the embeddings. Why do we need to perform this projection before visualizing the embeddings? \n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.6 (10 pts):\n",
    "Run the below cell to compute and visualize a 2-d projection of some words (stored in the *words_to_visualize* variable) using principal component analysis (if interested, see https://en.wikipedia.org/wiki/Principal_component_analysis). Make sure you have **matplotlib** installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "words_to_visualize = ['movie', 'film', 'watch', 'popcorn', 'director',\n",
    "                     'the', 'and', 'of', 'what', 'that', 'baseball',\n",
    "                     'mlb', 'nfl', 'sports', 'basketball']\n",
    "vecs = []\n",
    "for w in words_to_visualize:\n",
    "    try:\n",
    "        vecs.append(W[vocab[w]])\n",
    "    except:\n",
    "        print('%s does not exist in vocabulary :(' % w)    \n",
    "    \n",
    "small_W = np.array(vecs)\n",
    "W_proj = PCA(n_components=2).fit_transform(small_W) # apply t-SNE\n",
    "\n",
    "x = W_proj[:, 0] # first dimension\n",
    "y = W_proj[:, 1] # second dimension\n",
    "\n",
    "# plot the projected embeddings\n",
    "plt.scatter(x, y)\n",
    "plt.title('Word embeddings')\n",
    "\n",
    "# set up axes to show all words \n",
    "plt.xlim(x.min()-0.5, x.max()+0.5)\n",
    "plt.ylim(y.min()-0.5, y.max()+0.5)\n",
    "\n",
    "# put text markers on plot\n",
    "for w, x, y in zip(words_to_visualize, x, y):\n",
    "        plt.annotate(w, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to play around with this function by choosing 15-20 different words to include in *words_to_visualize* and seeing if any meaningful clusters emerge in the resulting plot (or not!). After experimenting, pick one plot that demonstrates both *syntactic* and *semantic* similarity between words. Justify your choice by giving examples of each type of similarity from the plot.\n",
    "\n",
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
